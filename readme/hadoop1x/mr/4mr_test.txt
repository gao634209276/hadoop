1. 目录
1	Mapper类和Reduce类	1
1.1	Mapper类	1
1.1.1	Mapper类的结构	2
1.1.2	Mapper的方法	2
1.1.3	源码java.doc	2
1.2	Reduce类	3
1.2.1	功能说明	3
1.2.2	java.doc	4
1.3	Context内部类	5
2	MapReduce初级案例二	5
3	MapReduce最小驱动配置	6
3.1	最小配置的MapReduce代码演示:	6
3.1.1	最小配置的输出	6
3.2	源代码查看MapReduce默认的设置	7
3.3	默认Mapper类	8
3.4	Mapper输出默认设置	9
3.4.1	MapOutputKeyClass	9
3.4.2	MapOutputValue	10
3.5	Reducer Class	10
3.6	Reducer 输出	11
3.7	Default MapReduce Job	11
3.8	MapReduce 模板	12

1 Mapper类和Reduce类
内容提纲
MapReduce中的基类Mapper类,自定义Mapper类的父类
MapReduce中的基类Reduce类,自定义Reduce类的父类

1.1 Mapper类
API文档:
1) InputSplit输入分片,InputFormat输入格式化
2) 对Mapper输入结果进行Sorted排序和Group分组
3) 对Mapper数据结果依据Redeucer个数进行分区Partition
4) 对Mapper数据数据进行Combiner

1.1.1 Mapper类的结构

1.1.2 Mapper的方法
(一) protected类型
1) setup():每个任务执行前调用一次, 对Map Task进行一些预处理
2) map():每个Key/Value对调用一次,每次接收一个Key/Value对其进行处理,再分发处理
3) cleanup():任务执行结束调用一次,对Map Task进行一些处理后的工作
(二) 运行方法
run()方法,是Mapper类的入口,方法内部调用类setup(), map(), cleanup()三个方法.相当于Map Task的驱动.
(1) 首先调用setup()方法进行初始化操作
(2) 然后对每个context.nextKeyValue()获取的Key-Value对,就调用map()方法进行处理
(3) 最后调用cleanup()做最后处理.
public void run(Context context) throws IOException, InterruptedException {
    setup(context);
    try {
      while (context.nextKeyValue()) {
        map(context.getCurrentKey(), context.getCurrentValue(), context);
      }
    } finally {
      cleanup(context);
    }
  }
run()方法提供了set()map()cleanup()的执行模板
1.1.3 源码java.doc
The Hadoop Map-Reduce framework spawns one map task for each InputSplit generated by the InputFormat for the job. Mapper implementations can access the Configuration for the job via the JobContext.getConfiguration().
job经过InputFormat后会被逐行分成一个个的输入分片InputSplit,然后Hadoop MapReduce框架为每一个输入分片InputSplit分一个map task,实例化的Mapper能获取到job的Configuration通过JobContext.getConfuguration()方法.

The framework first calls setup(org.apache.hadoop.mapreduce.Mapper.Context), followed by map(Object, Object, Context) for each key/value pair in the InputSplit. Finally cleanup(Context) is called.
这句话是run()方法的阐述.

All intermediate values associated with a given output key are subsequently grouped by the framework, and passed to a Reducer to determine the final output. Users can control the sorting and grouping by specifying two key RawComparator classes.
所有的中间键值对将被框架自动分组通过制定的Key,然后传递给Redcuer来决定最终的输出.用户可以通过指定主要的原始比较器类RawComparator来控制框架的排序和分组.

The Mapper outputs are partitioned per Reducer. Users can control which keys (and hence records) go to which Reducer by implementing a custom Partitioner.
Mapper的输出将会按照每个Reducer分区.用户可以控制哪些keys(和records)传输到哪些Reducer,通过实现一个自定义的Partitioner.

Users can optionally specify a combiner, via Job.setCombinerClass(Class), to perform local aggregation of the intermediate outputs, which helps to cut down the amount of data transferred from the Mapper to the Reducer.
用户可以指定一个conbiner,通过Job.setCombinerClass(Class),用来对本地的中间输出进行聚合,这将会缩减从Mapper传输到Reducer的大量数据.

Applications can specify if and how the intermediate outputs are to be compressed and which CompressionCodecs are to be used via the Configuration.
应用程序可以指定是否和怎样去压缩输出,以及使用Configuration的哪一个CompressionCodecs

If the job has zero reduces then the output of the Mapper is directly written to the OutputFormat without sorting by keys.
如果job没有reduces任务,那么Mapper的输出将直接写入到OutputFormat,而不用对Key进行sort.
1.2 Reduce类
Reduce类结构与Mapper类极为相似,只将其中的map()换做reduce()方法,此处不再重述.
1.1.4 功能说明
1. 获取map()方法输出的中间结果
2. 将中间结果中的Value按照Key划分组(group),而group按照Key排序,形成<key,(collection of values)>的结构,此时Key是唯一的.
The framework merge sorts Reducer inputs by keys (since different Mappers may have output the same key).
The shuffle and sort phases occur simultaneously i.e. while outputs are being fetched they are merged.
3. 处理group中的所有Value,相同Key的Value组合.最终Key对应的Value唯一,<key,value>序对形成.


1.1.5 java.doc

Reduces a set of intermediate values which share a key to a smaller set of values.
Recude类,将一个小的value set合并为一个大的set,并表示为中间值values.
Reducer implementations can access the Configuration for the job via the JobContext.getConfiguration() method.
实现的Reducer可以获取job的Configuration,通过JobContext.getConfiguration()方法.
Reducer has 3 primary phases:
1. Shuffle
The Reducer copies the sorted output from each Mapper using HTTP across the network.
第一阶段:洗牌Shuffle
Reducer复制被Mapper排序过的output,使用网络的HTTP协议.

2. Sort
The framework merge sorts Reducer inputs by keys (since different Mappers may have output the same key).
The shuffle and sort phases occur simultaneously i.e. while outputs are being fetched they are merged.
第二阶段排序:
不同的Mappers可能会输出相同的key,框架通过keys自动merge复制过来的键值对.
shuffle和sort阶段是同时出现的,也就是说输出键值对的时候就已经merge完成.

SecondarySort
To achieve a secondary sort on the values returned by the value iterator, the application should extend the key with the secondary key and define a grouping comparator. The keys will be sorted using the entire key, but will be grouped using the grouping comparator to decide which keys and values are sent in the same call to reduce.The grouping comparator is specified via Job.setGroupingComparatorClass(Class). The sort order is controlled by Job.setSortComparatorClass(Class).
二次排序
为了能对value iterator返回的Values进行二次排序,应用程序应继承含有二次Key的key,并且定义一个组比较器.Keys将会被排序使用entire key,但是不会组比较器进行分组,组比较器用于决定哪些keys和values会发往相同的reduce调用.组比较器可以通过Job.setGroupingGomparatorClass类来指定.而排序的规则用Job.setSortComparatorClass类控制.
For example, say that you want to find duplicate web pages and tag them all with the url of the "best" known example. You would set up the job like:
Map Input Key: url
Map Input Value: document
Map Output Key: document checksum, url pagerank
Map Output Value: url
Partitioner: by checksum
OutputKeyComparator: by checksum and then decreasing pagerank
OutputValueGroupingComparator: by checksum
3. Reduce
In this phase the reduce(Object, Iterable, Context) method is called for each <key, (collection of values)> in the sorted inputs.
The output of the reduce task is typically written to a RecordWriter via Context.write(Object, Object).
The output of the Reducer is not re-sorted.
第三阶段,Reduce归约
在这个阶段,reduce()方法将被,排过序的input中的每个<key,value集合>调用
redce任务的输出写入到一个RecordWriter中,通过Context.write()方法.
Reducer端的输出没有重排序.
1.3 Context内部类
Mapper和Reducer都有一内部类Context类,包含job所有资源参数的上下文.
在各自的run方法中调用context对象相关方法获取参数
run方法中while循环条件使用context.nextKeyValue()为MapContext/ReduceContext类中的方法,返回该类中的属性RecordReader<KEYIN,VALUEIN> reader; RecordReader为记录器, KEYIN为行数, VALUEIN资源的每一行. MapContext/ReduceContext仅仅是上下文参数的封装类,只是传递参数的类,不做任何操作;while循环体中为:map (context.getCurrentKey(), context.getCurrentValue(), context);均通过context来获取参数.
2 MapReduce初级案例二
Hadoop TopKey算法实现
一、 某个文件中牟列数据的最值(最大或者最小)
二、 某个文件中某列数据的Top Key值(最大或者最小)
三、 文件中某列数据的Top Key值(最大或最小)
四、 统计和Top Key
数据格式:
语言类别 歌曲名称 收藏次数 播放次数 歌手名称
需求
统计前十首播放次数最多的歌曲名称和次数
代码练习
一 ,1某个文件中某列的最大值
思路:对每一列的值依次进行比较,保存最大的值进行输出,算法思想类似于排序算法(快速和冒泡排序).
MapReduce:

四.略
3 MapReduce最小驱动配置
1.4 最小配置的MapReduce代码演示:

3.1.1 最小配置的输出
通过测试可以得到如下结论:
读取输入文件中的内容,输入到指定目录的输入文件中,此时文件中的内容为
Key:输入文件每行内容的起始位置
Value:输入文件每行的原始内容

输出文件的内容就是:Key+\t+Value


1.5 源代码查看MapReduce默认的设置
从代码中查看:先是配置Configuration,这个都是获取xml配置,与在代码中手动配置的最小化配置没有影响.接下来是实例化Job,
Job job = new Job(conf,MinimalMapReduce.class.getSimpleName());由于MinimalMapReduce.class中配置的为最小化,也就是说很多项都没有配置,在实例化的时候,系统会判断自己补上,所以说问题就要从这里下手寻找默认参数.
从job加载conf中寻找
’
Job(conf,jobName)调用了父类中的方法.也就会这里

JobContext类为Job的上下文类,如果,有些参数没有配置,那么上下文是怎么获取到的呢?


1.6 默认Mapper类
按照正常,配置一个Mapper类,如果没手动配置,查看JobContext的获取方法:
public Class<? extends Mapper<?,?,?,?>> getMapperClass()
     throws ClassNotFoundException {
    return (Class<? extends Mapper<?,?,?,?>>)
      conf.getClass(MAP_CLASS_ATTR, Mapper.class);
  }

返回值中调用类conf.getClass()内部有两个参数,显然这个方法一个是手工设定的类,一个是默认的类.如下代码:
public Class<?> getClass(String name, Class<?> defaultValue) {
    String valueString = get(name);
    if (valueString == null)
      return defaultValue;
    try {
      return getClassByName(valueString);
    } catch (ClassNotFoundException e) {
      throw new RuntimeException(e);
    }
  }
方法传参defaultValue就是getMapperClass()返回值中设定的Mapper.class所以,此时Mapper的默认值就是Mapper类.
1.7 Mapper输出默认设置
3.1.2 MapOutputKeyClass
从JobContext的getMapOutputKeyClass()查看:

  public Class<?> getMapOutputKeyClass() {
    return conf.getMapOutputKeyClass();
  }
public Class<?> getMapOutputKeyClass() {
    Class<?> retv = getClass("mapred.mapoutput.key.class", null, Object.class);
    if (retv == null) {
      retv = getOutputKeyClass();
    }
    return retv;
  }

public Class<?> getOutputKeyClass() {
    return getClass("mapred.output.key.class",
                    LongWritable.class, Object.class);
  }
经过一些列的调用传递,这里传入一个LongWritable 和Object类型调用下面的getClass()方法,在这里可以看到返回调用的MapOutputKeyClass的默认类型为LongWritable.class

public <U> Class<? extends U> getClass(String name,
                                         Class<? extends U> defaultValue,
                                         Class<U> xface) {
    try {
      Class<?> theClass = getClass(name, defaultValue);
      if (theClass != null && !xface.isAssignableFrom(theClass))
        throw new RuntimeException(theClass+" not "+xface.getName());
      else if (theClass != null)
        return theClass.asSubclass(xface);
      else
        return null;
    } catch (Exception e) {
      throw new RuntimeException(e);
    }
  }
3.1.3 MapOutputValue
做简单代码演示:
从JobContext类的getMapOutputValueClass() {
return conf.getMapOutputValueClass();
 Class<?> retv = getClass("mapred.mapoutput.value.class", null,Object.class);
    if (retv == null) {
      retv = getOutputValueClass();
    }
getClass(String name, Class<? extends U> defaultValue, Class<U> xface) {
    try {
      Class<?> theClass = getClass(name, defaultValue);
      if (theClass != null && !xface.isAssignableFrom(theClass))
        throw new RuntimeException(theClass+" not "+xface.getName());
      else if (theClass != null)
        return theClass.asSubclass(xface);
 getClass(String name, Class<?> defaultValue) {
    String valueString = get(name);
    if (valueString == null)
      return defaultValue;
    try {
      return getClassByName(valueString);}
经过着一系列调用,retv的值为null,再看getOutputValueClass():
 getOutputValueClass() {
    return getClass("mapred.output.value.class", Text.class, Object.class);
  }
这个又一次调用带三个参数的getClass(),必然结果仍然是第二个,也就是Text.class
所以MapOutputValue的默认输出类型为Text.class
1.8 Reducer Class
源码:
getReducerClass()
     throws ClassNotFoundException {
    return (Class<? extends Reducer<?,?,?,?>>)
      conf.getClass(REDUCE_CLASS_ATTR, Reducer.class);
 getClass(String name, Class<?> defaultValue) {
    String valueString = get(name);
    if (valueString == null)
      return defaultValue;
    try {
      return getClassByName(valueString);
这里的 REDUCE_CLASS_ATTR有一下声明
 protected static final String REDUCE_CLASS_ATTR = "mapreduce.reduce.class";
由于没有设置mapreduce.reduce.class,所以return defaultValue也就会Reducer.class
同Mapper,Reduce的默认类型为Reducer.class
1.9 Reducer 输出
这里同Mapper的输出方法相似. getOutputKeyClass() 和getOutputValueClass()在从Mapper输出的源码查询中已经查过了
结果和Mapper的输出一样Key类型和Value类型分别为: LongWritable.class和Text.class

结论:
默认情况下,reduce输出的Key/Value类型与输出的Key/Value类型相同
通过JobContext跟踪可以依次查出默认的配置如下:


1.10 Default MapReduce Job

public class DefaultMapReduce {

	public static void main(String[] args) throws Exception {

		args = new String[] { "hdfs://hadoop-master.dragon.org:9000/opt/test/input",
				"hdfs://hadoop-master.dragon.org:9000/opt/test/output" };
		// step 1: configuration
		Configuration conf = new Configuration();
		// step 2:create job
		Job job = new Job(conf, DefaultMapReduce.class.getSimpleName());
		// step 3: set job

		// 1: set run jar class
		job.setJarByClass(DefaultMapReduce.class);
		// 2: set input format
		job.setInputFormatClass(TextInputFormat.class);
		// 3: set input path
		FileInputFormat.addInputPath(job, new Path(args[0]));
		// 4: set mapper
		job.setMapperClass(Mapper.class);
		// 5: set map output key/value class
		job.setMapOutputKeyClass(LongWritable.class);
		job.setMapOutputValueClass(Text.class);
		// 6: set partitioner class
		job.setPartitionerClass(HashPartitioner.class);
		// 7: set reducer task number
		job.setNumReduceTasks(1);
		// 8: set sort comparator class
		job.setSortComparatorClass(LongWritable.Comparator.class);
		// 9: set group comparator class
		job.setGroupingComparatorClass(LongWritable.Comparator.class);
		// 10: set combiner class
		// job.setCombinerClass(null);
		// 11: set reduce class
		job.setReducerClass(Reducer.class);
		// 12: set output format
		job.setOutputFormatClass(TextOutputFormat.class);
		// 13: job output key/value class
		job.setOutputKeyClass(LongWritable.class);
		job.setOutputValueClass(Text.class);
		// 14:set job output path
		FileOutputFormat.setOutputPath(job, new Path(args[1]));

		// step 4 submit job
		boolean isSuccess = job.waitForCompletion(true);

		// step 5 exit
		System.exit(isSuccess ? 0 : 1);
	}
}
1.11 MapReduce 模板
/**
 * MapReduce 模板
 */
public class ModuleMapReduce extends Configured implements Tool {
	/**
	 * Mapper Class
	 */
	public static class MpduleMapper extends Mapper<LongWritable, Text, Text, LongWritable> {

		@Override
		public void cleanup(Context context) throws IOException, InterruptedException {
			super.cleanup(context);
		}

		@Override
		public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
			super.map(key, value, context);
		}

		@Override
		public void setup(Context context) throws IOException, InterruptedException {
			super.setup(context);
		}

	}

	/**
	 * Reduce Class
	 */
	public static class ModuleReducer extends Reducer<LongWritable, Text, Text, LongWritable> {

		@Override
		public void cleanup(Context context) throws IOException, InterruptedException {
			super.cleanup(context);
		}

		@Override
		public void reduce(LongWritable key, Iterable<Text> values, Context context)
				throws IOException, InterruptedException {
			super.reduce(key, values, context);
		}

		@Override
		public void setup(Context context) throws IOException, InterruptedException {
			super.setup(context);
		}

	}

	/**
	 * Driver
	 */
	@Override
	public int run(String[] args) throws Exception {

		// step 1:set configuration
		Configuration conf = new Configuration();

		Job job = parseInputAndOutput(this, conf, args);

		// step 3: set job
		// 1: set run jar class
		job.setJarByClass(ModuleMapReduce.class);
		// 2: set input format
		job.setInputFormatClass(TextInputFormat.class);
		// 3: set input path
		FileInputFormat.addInputPath(job, new Path(args[0]));
		// 4: set mapper
		job.setMapperClass(Mapper.class);
		// 5: set map output key/value class
		job.setMapOutputKeyClass(LongWritable.class);
		job.setMapOutputValueClass(Text.class);
		// 6: set partitioner class
		// job.setPartitionerClass(HashPartitioner.class);
		// 7: set reducer task number
		job.setNumReduceTasks(1);
		// 8: set sort comparator class
		// job.setSortComparatorClass(LongWritable.Comparator.class);
		// 9: set group comparator class
		// job.setGroupingComparatorClass(LongWritable.Comparator.class);
		// 10: set combiner class
		// job.setCombinerClass(null);
		// 11: set reduce class
		job.setReducerClass(Reducer.class);
		// 12: set output format
		job.setOutputFormatClass(TextOutputFormat.class);
		// 13: job output key/value class
		job.setOutputKeyClass(LongWritable.class);
		job.setOutputValueClass(Text.class);
		// 14:set job output path
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		// step 4 submit job
		boolean isSuccess = job.waitForCompletion(true);
		return isSuccess ? 0 : 1;

	}

	public Job parseInputAndOutput(Tool tool, Configuration conf, String[] args) throws Exception {

		// validate
		if (args.length != 2) {
			System.err.printf("Usage: %s [generic options] <input> <output> \n");
			return null;
		}

		// step 2:create job
		Job job = new Job(conf, tool.getClass().getSimpleName());
		return job;
	}

	public static void main(String[] args) throws Exception {
		// run map reduce
		int status = ToolRunner.run(new ModuleMapReduce(), args);
		// step 5 exit
		System.exit(status);
	}
}

1.12 依据模板MapReduce类编写WordCount程序
3.1.4 修改名称
(MapReduce类名称,Mapper类的名称和Reduce类的名称)
选中要改的类名/属性名/方法名 Ctrl+Alt+R+Enter,进入重构修改.
3.1.5 K/V输入输出参数类型
依据实际的业务逻辑,修改Mapper类和Reduce类的Key/Value输入输出参数类型.
3.1.6 修改驱动Driver输入输出类型
Mapper类和Reducer类的输出类型
3.1.7 在Mapper类中编写实际业务逻辑
setup(), map(), clearup()
		private final static IntWritable one = new IntWritable(1);
		private Text word = new Text();
			StringTokenizer itr = new StringTokenizer(value.toString());
			while (itr.hasMoreTokens()) {
				word.set(itr.nextToken());
				context.write(word, one);
3.1.8 在Reducer类中编写实际代码
setup(), reduce(), clearup()
		private IntWritable result = new IntWritable();
			int sum = 0;
			for (IntWritable value : values) {
				sum += value.get();
			}
			result.set(sum);
			context.write(key, result);
3.1.9 检查并修改驱动代码
模板类中的run()方法
3.1.10 设置输入输出路径,进行MR测试
4 MapReduce计数器
1.13 运行官方WordCount MapReduce程序
通过Jsp UI管理界面50030查看Map/Reduce

在首页界面会出现一个Running Map Tasks列表,点击能够进入以上界面(部分截图).
其中Job Counters		Job计数器.
File Output Format Counts	输出
File Input Format Counts	输入
FileSystemCounters			文件系统
Map-Reduce Framework		Map-Reduce框架
四个部分
1.14 在shell中显示日志信息
穿插中文解析:
[hadoop@hadoop-master hadoop-1.2.1]$ hadoop jar hadoop-examples-1.2.1.jar wordcount /opt/wc/input /opt/wc/output
16/04/26 08:51:01 INFO input.FileInputFormat: Total input paths to process : 3
文件输入格式:总共输入路径到程序有3个
16/04/26 08:51:01 INFO util.NativeCodeLoader: Loaded the native-hadoop library
本地代码加载器:本地hadoop 库已经加载
这个库位置在/opt/modules/hadoop-1.2.1/c++/Linux-amd64-64/lib目录中,主要是so文件
16/04/26 08:51:01 WARN snappy.LoadSnappy: Snappy native library not loaded
警告: 加载压缩算法,本地库没有加载
然后开始运行Job,运行Job相关进度
16/04/26 08:51:01 INFO mapred.JobClient: Running job: job_201604260441_0001
运行Job的ID
16/04/26 08:51:02 INFO mapred.JobClient:  map 0% reduce 0%
16/04/26 08:51:07 INFO mapred.JobClient:  map 66% reduce 0%
16/04/26 08:51:08 INFO mapred.JobClient:  map 100% reduce 0%
16/04/26 08:51:14 INFO mapred.JobClient:  map 100% reduce 33%
16/04/26 08:51:15 INFO mapred.JobClient:  map 100% reduce 100%
Job运行时Map Task和Reduce Task的运行进度
16/04/26 08:51:15 INFO mapred.JobClient: Job complete: job_201604260441_0001
Job运行完成
显示整个Job运行过程中,各类计算器Counter的值
16/04/26 08:51:15 INFO mapred.JobClient: Counters: 29
总共有29种计数器Counters
16/04/26 08:51:15 INFO mapred.JobClient:   Job Counters
16/04/26 08:51:15 INFO mapred.JobClient:     Launched reduce tasks=1
16/04/26 08:51:15 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=7305
16/04/26 08:51:15 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
16/04/26 08:51:15 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
16/04/26 08:51:15 INFO mapred.JobClient:     Launched map tasks=3
16/04/26 08:51:15 INFO mapred.JobClient:     Data-local map tasks=3
16/04/26 08:51:15 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=8499
这一部分为Job计数器Counters个数:7个
16/04/26 08:51:15 INFO mapred.JobClient:   File Output Format Counters
16/04/26 08:51:15 INFO mapred.JobClient:     Bytes Written=85
输出类型计数器counters个数1个
16/04/26 08:51:15 INFO mapred.JobClient:   FileSystemCounters
16/04/26 08:51:15 INFO mapred.JobClient:     FILE_BYTES_READ=303
16/04/26 08:51:15 INFO mapred.JobClient:     HDFS_BYTES_READ=576
16/04/26 08:51:15 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=218366
16/04/26 08:51:15 INFO mapred.JobClient:     HDFS_BYTES_WRITTEN=85
FS文件系统计数器:4个
16/04/26 08:51:15 INFO mapred.JobClient:   File Input Format Counters
16/04/26 08:51:15 INFO mapred.JobClient:     Bytes Read=210
输入类型计数器:1个
以下全部为Map-Reduce框架计数器:16个
16/04/26 08:51:15 INFO mapred.JobClient:   Map-Reduce Framework
16/04/26 08:51:15 INFO mapred.JobClient:     Map output materialized bytes=315
16/04/26 08:51:15 INFO mapred.JobClient:     Map input records=16
16/04/26 08:51:15 INFO mapred.JobClient:     Reduce shuffle bytes=315
16/04/26 08:51:15 INFO mapred.JobClient:     Spilled Records=42
16/04/26 08:51:15 INFO mapred.JobClient:     Map output bytes=304
16/04/26 08:51:15 INFO mapred.JobClient:     Total committed heap usage (bytes)=453509120
16/04/26 08:51:15 INFO mapred.JobClient:     CPU time spent (ms)=2550
16/04/26 08:51:15 INFO mapred.JobClient:     Combine input records=24
16/04/26 08:51:15 INFO mapred.JobClient:     SPLIT_RAW_BYTES=366
16/04/26 08:51:15 INFO mapred.JobClient:     Reduce input records=21
16/04/26 08:51:15 INFO mapred.JobClient:     Reduce input groups=9
16/04/26 08:51:15 INFO mapred.JobClient:     Combine output records=21
16/04/26 08:51:15 INFO mapred.JobClient:     Physical memory (bytes) snapshot=648404992
16/04/26 08:51:15 INFO mapred.JobClient:     Reduce output records=9
16/04/26 08:51:15 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=3045171200
16/04/26 08:51:15 INFO mapred.JobClient:     Map output records=24

这些在shell输入信息在一下代码中设置
boolean isSuccess = job.waitForCompletion(true);
1.15 计数器Counter
Job Counters , File Output Format Counters , FileSystemCounters , File Input Format Counters , Map-Reduce Framework
MapReduce计数器(Counter)为我们提供一个窗口,用于观察MapReduce Job运行期的各种细节数据
MapReduce Framwork包含类箱单多的Job执行细节数据,一般情况下,record表示行数据,byte表示行数据所占字节数

4.1.1 输入输出计数器:

输入从文件系统中读取的字节数:

44+61-105=210;
输出是写到程序运行完成写到文件系统中的字节数:正好85字节


4.1.2 文件系统计数器
在进行MapReduce的过程中文件读写有一下几个步骤:
HDFS_BYTES_READ	FILE_BYTES_WRITTEN	FILE_BYTES_READ	HDFS_BYTES_WRITTEN
576字节			163.707字节			576字节			85字节
HDFS-Read		File-Write				File-Read		HDFS-Write
map():read		map():write			reduce():read		reduce():write
		Mapper端							Reducer端

